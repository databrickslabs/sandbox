import dlt
from dbruntime.dbutils import FileInfo
import re

# Dynamic Tables
def sanitize_table_name(name: str) -> str:
  """
  Make a valid, reasonably human-friendly table name from a folder name.
  - Lowercase
  - Replace non [a-z0-9_] with underscores
  - Ensure it doesn't start with a digit
  """
  n = name.strip().lower()
  n = re.sub(r"[^a-z0-9_]", "_", n)
  if re.match(r"^[0-9]", n):
    n = f"t_{n}"
  n = re.sub(r"_+", "_", n).strip("_")
  return n or "t_unnamed"

def dbfs_is_dir(f: FileInfo):
  is_dir_attr = getattr(f, "isDir", None)
  return is_dir_attr() if callable(is_dir_attr) else f.name.endswith("/")

def list_immediate_subdirs(path: str):
  items = dbutils.fs.ls(path)
  out = []
  for f in items:
    if dbfs_is_dir(f):
      # f.name often ends with '/', drop it for a clean folder name
      clean_name = f.name[:-1] if f.name.endswith("/") else f.name
      out.append((clean_name, f.path.removeprefix('dbfs:')))
  return out

def make_dlt_table(subdir_name: str, subdir_path: str):
  """
  Defines a DLT table for a given subfolder at import time.
  Uses Auto Loader (streaming) if `streaming=True`, else batch reader.
  """

  table_name = sanitize_table_name(subdir_name)

  if len(dbutils.fs.ls(subdir_path)) > 0:
    @dlt.table(
      name=table_name,
      comment=f"Auto-created from subfolder: {subdir_path} (streaming via Auto Loader)",
      table_properties={
        "volume_path": f"{subdir_path}"
      }
    )
    def _auto_loader_table():
      return spark.readStream.format("cloudFiles").option("cloudFiles.format","csv").load(subdir_path)
  
for subdir_name, subdir_path in list_immediate_subdirs('{{template `volume_path` .}}'):
  make_dlt_table(subdir_name, subdir_path)