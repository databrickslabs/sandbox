# Databricks notebook source
# MAGIC %md
# MAGIC # sql2dbx
# MAGIC sql2dbx is an automation tool designed to convert SQL files into Databricks notebooks. It leverages Large Language Models (LLMs) to perform the conversion based on system prompts tailored for various SQL dialects. sql2dbx consists of a series of Databricks notebooks.
# MAGIC
# MAGIC While the Databricks notebooks generated by sql2dbx may require manual adjustments, they serve as a valuable starting point for migrating SQL-based workflows to the Databricks environment.
# MAGIC
# MAGIC This main notebook functions as the entry point for sql2dbx's series of processes that convert SQL files into Databricks notebooks. It also includes an experimental feature to convert the resulting Python notebooks to SQL notebooks.

# COMMAND ----------

# MAGIC
# MAGIC %md-sandbox
# MAGIC ## üí´ Conversion Flow Diagram
# MAGIC The diagram below illustrates the flow of the SQL to Databricks notebooks conversion process.
# MAGIC
# MAGIC     <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
# MAGIC     <script>
# MAGIC         mermaid.initialize({startOnLoad:true});
# MAGIC     </script>
# MAGIC     <div class="mermaid">
# MAGIC       flowchart TD
# MAGIC           input[Input SQL Files] -->|Input| analyze[[01_analyze_input_files]]
# MAGIC           analyze <-->|Read & Write| conversionTable[Conversion Result Table]
# MAGIC
# MAGIC           conversionTable <-->|Read & Write| convert[[02_convert_sql_to_databricks]]
# MAGIC           convert -.->|Use| endpoint[Model Serving Endpoint]
# MAGIC           convert -.->|Refer| prompts["Conversion Prompt Yaml<br/>(SQL Dialect Specific)"]
# MAGIC
# MAGIC           conversionTable <-->|Read & Write| validate[[03_01_static_syntax_check]]
# MAGIC
# MAGIC           conversionTable <-->|Read & Write| fixErrors[[03_02_fix_syntax_error]]
# MAGIC           fixErrors -.->|Use| endpoint
# MAGIC
# MAGIC           conversionTable <-->|Read & Write| splitCells[[04_split_cells]]
# MAGIC
# MAGIC           conversionTable -->|Input| export[[05_export_to_databricks_notebooks]]
# MAGIC           export -->|Output| notebooks[Converted Databricks Notebooks]
# MAGIC
# MAGIC           conversionTable <-->|Read & Write| adjust[[11_adjust_conversion_targets]]
# MAGIC
# MAGIC           %% Layout control with invisible lines
# MAGIC           convert --- validate --- fixErrors --- splitCells --- export
# MAGIC
# MAGIC           %% Styling
# MAGIC           classDef process fill:#E6E6FA,stroke:#333,stroke-width:2px;
# MAGIC           class analyze,convert,validate,fixErrors,splitCells,adjust,export process;
# MAGIC           classDef data fill:#E0F0E0,stroke:#333,stroke-width:2px;
# MAGIC           class input,conversionTable,notebooks data;
# MAGIC           classDef external fill:#FFF0DB,stroke:#333,stroke-width:2px;
# MAGIC           class endpoint,prompts external;
# MAGIC
# MAGIC           %% Make layout control lines invisible
# MAGIC           linkStyle 12 stroke-width:0px;
# MAGIC           linkStyle 13 stroke-width:0px;
# MAGIC           linkStyle 14 stroke-width:0px;
# MAGIC           linkStyle 15 stroke-width:0px;
# MAGIC     </div>

# COMMAND ----------

# MAGIC %md
# MAGIC ## üìù Conversion Steps
# MAGIC This main notebook executes the following notebooks in sequence:
# MAGIC
# MAGIC | Notebook Name | Description |
# MAGIC |---|---|
# MAGIC | <a href="$./01_analyze_input_files" target="_blank">01_analyze_input_files</a> | Analyzes the input SQL files, calculates token counts, and saves the results to a Delta table. |
# MAGIC | <a href="$./02_convert_sql_to_databricks" target="_blank">02_convert_sql_to_databricks</a> | Converts the SQL code to a Python function that runs in a Databricks notebook using an LLM and updates the result table. |
# MAGIC | <a href="$./03_01_static_syntax_check" target="_blank">03_01_static_syntax_check</a> | Performs static syntax checks on Python functions and the Spark SQL contained within them, updating the result table with any errors found. |
# MAGIC | <a href="$./03_02_fix_syntax_error" target="_blank">03_02_fix_syntax_error</a> | Fixes syntax errors in Python functions and SQL statements identified in the previous step using an LLM and updates the result table. |
# MAGIC | <a href="$./04_split_cells" target="_blank">04_split_cells</a> | Splits the converted code into separate cells for better organization and readability in Databricks notebooks. |
# MAGIC | <a href="$./05_export_to_databricks_notebooks" target="_blank">05_export_to_databricks_notebooks</a> | Exports the converted code to Databricks notebooks. |
# MAGIC | <a href="$./06_convert_to_sql_notebooks" target="_blank">06_convert_to_sql_notebooks</a> | (Experimental) Converts Python notebooks to SQL notebooks using an LLM. This is executed only when `sql_output_dir` is specified. |
# MAGIC | <a href="$./11_adjust_conversion_targets" target="_blank">11_adjust_conversion_targets</a> | (Optional) Adjusts the conversion targets by setting the `is_conversion_target` field to `True` for specific files that need to be re-converted. This can be used to reprocess files that did not convert satisfactorily. |
# MAGIC
# MAGIC ## üéØ Conversion Sources
# MAGIC sql2dbx provides conversion from various SQL dialects to Databricks notebooks through YAML-defined conversion prompts. The table below shows the default YAML file for each supported SQL dialect. When you select a specific dialect in the `sql_dialect` widget, the corresponding default YAML file is used automatically. If you specify a path in the `conversion_prompt_yaml` widget, it will take precedence over the default YAML for that dialect.
# MAGIC
# MAGIC | SQL Dialect | Source System Example | Default YAML file |
# MAGIC | --- | --- | --- |
# MAGIC | `mysql` | MySQL / MariaDB / Amazon Aurora MySQL | <a href="$./pyscripts/conversion_prompt_yaml/mysql_to_databricks_notebook.yml" target="_blank">mysql_to_databricks_notebook.yml</a> |
# MAGIC | `netezza` | IBM Netezza | <a href="$./pyscripts/conversion_prompt_yaml/netezza_to_databricks_notebook.yml" target="_blank">netezza_to_databricks_notebook.yml</a> |
# MAGIC | `oracle` | Oracle Database / Oracle Exadata | <a href="$./pyscripts/conversion_prompt_yaml/oracle_to_databricks_notebook.yml" target="_blank">oracle_to_databricks_notebook.yml</a> |
# MAGIC | `postgresql` | PostgreSQL / Amazon Aurora PostgreSQL | <a href="$./pyscripts/conversion_prompt_yaml/postgresql_to_databricks_notebook.yml" target="_blank">postgresql_to_databricks_notebook.yml</a> |
# MAGIC | `redshift` | Amazon Redshift | <a href="$./pyscripts/conversion_prompt_yaml/redshift_to_databricks_notebook.yml" target="_blank">redshift_to_databricks_notebook.yml</a> |
# MAGIC | `snowflake` | Snowflake | <a href="$./pyscripts/conversion_prompt_yaml/snowflake_to_databricks_notebook.yml" target="_blank">snowflake_to_databricks_notebook.yml</a> |
# MAGIC | `teradata` | Teradata | <a href="$./pyscripts/conversion_prompt_yaml/teradata_to_databricks_notebook.yml" target="_blank">teradata_to_databricks_notebook.yml</a> |
# MAGIC | `tsql` | Azure Synapse Analytics / Microsoft SQL Server / Azure SQL Database / Azure SQL Managed Instance | <a href="$./pyscripts/conversion_prompt_yaml/tsql_to_databricks_notebook.yml" target="_blank">tsql_to_databricks_notebook.yml</a> |
# MAGIC
# MAGIC ### Creating Custom Conversion Prompts (Optional)
# MAGIC If you want to create a custom conversion prompt, you can create a YAML file in a structured format, place it in your Databricks workspace, and specify its full path in the `conversion_prompt_yaml` widget. This allows the LLM to reference the specified YAML file for the conversion. Custom conversion prompts require the following two sections:
# MAGIC 
# MAGIC 1. **`system_message`**: Required section that instructs the LLM on how to perform the conversion
# MAGIC 2. **`few_shots`**: Section showing specific input-output examples (optional but recommended)
# MAGIC 
# MAGIC #### Tips for Custom Conversion Prompts
# MAGIC Here are key points for creating effective conversion prompts:
# MAGIC 
# MAGIC **Elements to include in the `system_message`**:
# MAGIC - Clear explanation of the conversion purpose
# MAGIC - Definition of input and output formats
# MAGIC - Additional instructions needed for specific conversions
# MAGIC - (Optional but recommended) Comment language specification (`{comment_lang}` is automatically replaced with the specified language)
# MAGIC - (Optional but recommended) Reference to <a href="$./pyscripts/conversion_prompt_yaml/common_instructions/sql_to_databricks_notebook_common_python.yml" target="_blank">common instructions</a> for SQL to Databricks Notebook (Python) conversion (`${common_python_instructions_and_guidelines}`)
# MAGIC
# MAGIC **Effective use of `few_shots`**:
# MAGIC - Include typical cases ranging from simple to complex examples
# MAGIC - Ensure each example demonstrates specific patterns that help the LLM's understanding
# MAGIC
# MAGIC #### Example of a Custom Conversion Prompt
# MAGIC Below is a basic example of a YAML file for a custom conversion prompt:
# MAGIC
# MAGIC ```yaml
# MAGIC system_message: |
# MAGIC   Convert SQL code to Python code that runs on Databricks according to the following instructions:
# MAGIC
# MAGIC   # Input and Output
# MAGIC   - Input: A single SQL file containing one or multiple T-SQL statements (including but not limited to `CREATE OR ALTER PROCEDURE` statements).
# MAGIC   - Output: Python code with Python comments (in {comment_lang}) explaining the code and any necessary context.
# MAGIC
# MAGIC   ${common_python_instructions_and_guidelines}
# MAGIC
# MAGIC   # Additional Instructions
# MAGIC   1. Convert SQL queries to spark.sql() format
# MAGIC   2. Add clear Python comments explaining the code
# MAGIC   3. Use DataFrame operations instead of loops when possible
# MAGIC   4. Handle errors using try-except blocks
# MAGIC
# MAGIC few_shots:
# MAGIC - role: user
# MAGIC   content: |
# MAGIC     SELECT name, age
# MAGIC     FROM users
# MAGIC     WHERE active = 1;
# MAGIC - role: assistant
# MAGIC   content: |
# MAGIC     # Get names and ages of active users
# MAGIC     active_users = spark.sql("""
# MAGIC         SELECT name, age
# MAGIC         FROM users
# MAGIC         WHERE active = 1
# MAGIC     """)
# MAGIC     display(active_users)
# MAGIC ```
# MAGIC
# MAGIC ## üì¢ Prerequisites
# MAGIC Before running the main notebook, ensure that a Databricks model serving endpoint is available. You can either:
# MAGIC
# MAGIC 1. Use the Databricks Foundation Model APIs (recommended for the simplest setup)
# MAGIC 2. Set up an external model serving endpoint. You can configure it manually or use one of the following automation notebooks:
# MAGIC     - <a href="$./external_model/external_model_azure_openai" target="_blank">Notebook for Azure OpenAI Service Endpoint Setup</a>
# MAGIC     - <a href="$./external_model/external_model_amazon_bedrock" target="_blank">Notebook for Amazon Bedrock Endpoint Setup</a>
# MAGIC
# MAGIC ## ‚ùó Important Notes
# MAGIC The following points should be considered before running the main notebook:
# MAGIC
# MAGIC ### Model Compatibility
# MAGIC sql2dbx has been optimized for models with large context windows and strong SQL reasoning capabilities. The following models have been verified to produce highly accurate conversions:
# MAGIC
# MAGIC > **Note:** Model specifications are evolving rapidly. For the most current specifications, please check the official documentation for each model before implementation.
# MAGIC
# MAGIC #### Primary Recommendation
# MAGIC For optimal performance and minimal setup, we recommend using this model:
# MAGIC
# MAGIC | Model | API Model Version | Input Context | Max Output | Setup Requirement | Notes |
# MAGIC |---|---|---|---|---|---|
# MAGIC | [Claude 3.7 Sonnet](https://docs.anthropic.com/en/docs/about-claude/models/all-models) | claude-3-7-sonnet-20250219 | 200K tokens | Normal: 8,192 tokens<br>Extended thinking: 64,000 tokens | Ready to use via Foundation Model API | Best overall choice for complex SQL conversion |
# MAGIC
# MAGIC ##### Extended Thinking Mode
# MAGIC Claude 3.7 Sonnet's extended thinking mode is optional for simple SQL queries but is recommended for handling complex SQL conversions. To enable this feature, configure the `request_params` notebook widget with thinking parameters as shown below:
# MAGIC
# MAGIC Example:
# MAGIC ```json
# MAGIC {"max_tokens": 64000, "thinking": {"type": "enabled", "budget_tokens": 16000}}
# MAGIC ```
# MAGIC
# MAGIC > **Note:** Enabling extended thinking mode significantly improves the accuracy of complex SQL conversions but increases token usage and processing time. If the token count of the input SQL file (after removing SQL comments and extra whitespace) is 8,000 or less, extended thinking mode is expected to operate stably. For token counts exceeding this limit, errors may occur. When processing large SQL files, it is recommended to either split them into smaller files or perform the conversion without using extended thinking mode.
# MAGIC
# MAGIC #### Alternative Options for Azure Environments
# MAGIC Some organizations may have specific requirements to use Azure OpenAI models due to corporate policies or existing Azure investments. For these users, the following models are compatible:
# MAGIC
# MAGIC | Model | API Model Version | Input Context | Max Output | Setup Requirement | Notes |
# MAGIC |---|---|---|---|---|---|
# MAGIC | [Azure OpenAI o1](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning) | 2024-12-17 | 200K tokens | 100K tokens | Requires setup of External model | Good option for Azure environments |
# MAGIC | [Azure OpenAI o3-mini](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning) | 2025-01-31 | 200K tokens | 100K tokens | Requires setup of External model | Good option for Azure environments |
# MAGIC
# MAGIC ##### Notes for OpenAI O-series Models Parameters
# MAGIC When specifying `request_params`, there are several important considerations:
# MAGIC
# MAGIC 1. `reasoning_effort` parameter:
# MAGIC    - `reasoning_effort` controls the depth of reasoning in the inference process for O-series models.
# MAGIC    - By specifying `{"reasoning_effort": "high"}` in `request_param`, the model performs deeper reasoning, enabling more accurate conversion of complex SQL queries.
# MAGIC    - As a trade-off, token consumption and processing time will increase.
# MAGIC 1. Differences in token limit parameters:
# MAGIC    - For O-series models, the use of `max_tokens` is not recommended; instead, `max_completion_tokens` is used.
# MAGIC    - It is recommended to run sql2dbx without specifying `max_completion_tokens` for more stable operation.
# MAGIC 1. Unsupported parameters:
# MAGIC    - Generation parameters such as `temperature` and `top_p` are not supported by O-series models.
# MAGIC    - For more details, refer to the [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning).
# MAGIC
# MAGIC #### Other Compatible Models
# MAGIC The following models have also been verified to work with sql2dbx with varying performance characteristics:
# MAGIC
# MAGIC | Model | API Model Version | Input Context | Max Output | Setup Requirement | Notes |
# MAGIC |---|---|---|---|---|---|
# MAGIC | [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/about-claude/models/all-models) | claude-3-5-sonnet-20241022 | 200K tokens | 8,192 tokens | Requires setup of External model | Verified compatible |
# MAGIC | [Azure OpenAI GPT-4o](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) | 2024-05-13 | 128K tokens | 4,096 tokens | Requires setup of External model | Verified compatible |
# MAGIC | [Meta Llama 3.3 70B Instruct](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md) | llama-3-3-70b-instruct | 128K tokens | 8,192 tokens | Ready to use via Foundation Model API | Verified compatible |
# MAGIC
# MAGIC ### Token Management for SQL Conversion
# MAGIC The token count of SQL files directly impacts the conversion process. Two critical factors to consider:
# MAGIC
# MAGIC 1. The number of tokens in the generated Databricks notebook is typically larger than the number of tokens in the input SQL file. This is because the input SQL file reduces token count by removing comments and consolidating multiple spaces into one, while the generated notebook includes comments and indentation
# MAGIC 2. In the case of SQL files with extensive processing content, it may not be possible to generate the entire notebook in a single output. In such cases, the process will retain the already generated content and output the remainder in stages
# MAGIC
# MAGIC These factors make it essential to manage token usage efficiently. Files exceeding the model's effective processing capacity cannot be converted reliably, while staying within optimal thresholds ensures successful conversions.
# MAGIC
# MAGIC #### Recommended Token Thresholds for `token_count_threshold` Parameter
# MAGIC The `token_count_threshold` parameter determines which SQL files will be processed based on their token count (not file size). SQL content is tokenized after removing SQL comments and extra whitespace.
# MAGIC
# MAGIC | Model | Recommended `token_count_threshold` |
# MAGIC |---|---|
# MAGIC | Claude 3.7 Sonnet (Normal Mode) | 20,000 tokens (default) |
# MAGIC | Claude 3.7 Sonnet (Extended Thinking Mode) | 8,000 tokens |
# MAGIC
# MAGIC - The 20,000 token value for normal mode is based on actual testing results. While testing environments have successfully processed up to 60,000 tokens, stability decreases when exceeding 20,000 tokens. For the most stable operation, we have set 20,000 tokens as the default value.
# MAGIC - Other models (such as o1, o3-mini, etc.) have also been confirmed to operate relatively stably up to around 20,000 tokens. Theoretically, larger values may be possible, but we recommend testing in your actual environment.
# MAGIC - The 8,000 token limit for extended thinking mode is similarly derived from actual testing results. Exceeding this value may result in errors or no results being returned. When processing large SQL files, we recommend splitting them into smaller logical sections.
# MAGIC
# MAGIC #### Input File Token Count Process
# MAGIC The <a href="$./01_analyze_input_files" target="_blank">01_analyze_input_files</a> notebook counts tokens in input files as follows:
# MAGIC
# MAGIC 1. Determines the appropriate tokenizer based on the endpoint name or explicit tokenizer settings:
# MAGIC    - For Claude models: Uses a character-based estimation (approx. 3.4 characters per token) based on [Anthropic's documentation](https://docs.anthropic.com/en/docs/about-claude/models/all-models)
# MAGIC    - For OpenAI and other models: Uses the [openai/tiktoken](https://github.com/openai/tiktoken) library with appropriate encoding
# MAGIC 2. Measures token count of each SQL file using the selected tokenizer after removing SQL comments and extra whitespace
# MAGIC 3. Files with token count ‚â§ `token_count_threshold` are marked as conversion targets (`is_conversion_target = True`)
# MAGIC 4. Files exceeding the threshold are excluded from the conversion process
# MAGIC
# MAGIC ## üîå Parameters
# MAGIC The main notebook requires the following parameters to be set. For more granular parameter settings, please run individual specialized notebooks instead of this main notebook. The individual notebooks allow for more detailed customization for specific tasks.
# MAGIC
# MAGIC Index | Parameter Name | Required | Description | Default Value
# MAGIC --- | --- | --- | --- | ---
# MAGIC 1-1 | `input_dir` | Yes | The directory containing the SQL files to be converted. Supports locations accessible through Python `os` module (e.g., Unity Catalog Volume, Workspace, Repos, etc.). |
# MAGIC 1-2 | `endpoint_name` | Yes | The name of the Databricks Model Serving endpoint. You can find the endpoint name under the `Serving` tab. Example: If the endpoint URL is `https://<workspace_url>/serving-endpoints/hinak-oneenvgpt4o/invocations`, specify `hinak-oneenvgpt4o`. | `databricks-claude-3-7-sonnet`
# MAGIC 1-3 | `result_catalog` | Yes | The existing catalog where the result table will be stored. |
# MAGIC 1-4 | `result_schema` | Yes | The existing schema under the specified catalog where the result table will reside. |
# MAGIC 1-5 | `token_count_threshold` | Yes | Specifies the maximum token count allowed without SQL comments for files to be included in the following conversion process. | `20000`
# MAGIC 1-x | `existing_result_table` | No | The existing result table to use for storing the analysis results. If specified, the table will be used instead of creating a new one. |
# MAGIC 2-1 | `sql_dialect` | Yes | The SQL dialect of the input files. This parameter is used to determine the appropriate conversion prompts for the SQL dialect. | `tsql`
# MAGIC 2-2 | `comment_lang` | Yes | The language for comments to be added to the converted Databricks notebooks. | `English`
# MAGIC 2-3 | `concurrency` | Yes | The number of concurrent requests sent to the model serving endpoint. | `4`
# MAGIC 2-4 | `log_level` | Yes | The logging level to use for the batch inference process. Options are `INFO` for standard logging or `DEBUG` for detailed debug information. | `INFO`
# MAGIC 2-x | `request_params` | No | Additional chat request parameters for chat in JSON format, such as `{"max_tokens": 8192}` (see [Databricks Foundation Model APIs](https://docs.databricks.com/en/machine-learning/foundation-models/api-reference.html#chat-request) for reference). Empty value will use model's default parameters. |
# MAGIC 2-x | `conversion_prompt_yaml` | No | The path to the YAML file containing the conversion prompts. This file defines the system message and few-shot examples for the specific SQL dialect (e.g., T-SQL) to be converted. |
# MAGIC 3-1 | `max_fix_attempts` | Yes | The maximum number of attempts to automatically fix syntax errors in the conversion results. | `1`
# MAGIC 5-1 | `output_dir` | Yes | The directory where Databricks notebooks are saved. Supports the path in Workspace or Repos. |
# MAGIC 6-1 | `sql_output_dir` | No | (Experimental) The directory where SQL notebooks are saved. If specified, Python notebooks will be converted to SQL notebooks. Supports the path in Workspace or Repos. |
# MAGIC
# MAGIC ## üìÇ Input and Output
# MAGIC Main input and output of the conversion process are as follows:
# MAGIC
# MAGIC ### Input SQL Files
# MAGIC You should store the input SQL files in the `input_files_path` directory. <a href="$./01_analyze_input_files" target="_blank">01_analyze_input_files</a> notebook processes all files in the directory and its subdirectories. Supports locations accessible through Python `os` module (e.g., Unity Catalog Volume, Workspace, Repos, etc.).
# MAGIC
# MAGIC ### Conversion Result Notebooks (Final Output)
# MAGIC The conversion result Databricks notebooks are created by the <a href="$./05_export_to_databricks_notebooks" target="_blank">05_export_to_databricks_notebooks</a> notebook and serves as the final output of the conversion process. Supports a path in Workspace or Repos.
# MAGIC
# MAGIC ### SQL Notebooks (Optional Output)
# MAGIC If `sql_output_dir` is specified, the <a href="$./06_convert_to_sql_notebooks" target="_blank">06_convert_to_sql_notebooks</a> notebook (experimental) will convert the Python notebooks to SQL notebooks. This is an optional step that provides SQL-based versions of the converted notebooks.
# MAGIC
# MAGIC ### Conversion Result Table (Intermediate Output)
# MAGIC The conversion result table is created by the <a href="$./01_analyze_input_files" target="_blank">01_analyze_input_files</a> notebook and serves as both an input and output for subsequent notebooks in the conversion process. It is a Delta Lake table that stores the analysis results of input SQL files, including token counts, file metadata, and conversion status.
# MAGIC
# MAGIC #### Table Naming
# MAGIC The table name is constructed using parameters specified in the notebooks, following the format below:
# MAGIC
# MAGIC `{result_catalog}.{result_schema}.{result_table_prefix}_{YYYYMMDDHHmm}`
# MAGIC
# MAGIC For example, if the `result_catalog` is "my_catalog", the `result_schema` is "my_schema", the `result_table_prefix` is "conversion_targets", and the current time (UTC) is 2024-06-14 11:39, the table name will be:
# MAGIC
# MAGIC `my_catalog.my_schema.conversion_targets_202406141139`
# MAGIC
# MAGIC #### Table Schema
# MAGIC The table schema is as follows:
# MAGIC
# MAGIC | Column Name | Data Type | Description |
# MAGIC |---|---|---|
# MAGIC | `input_file_number` | int | A unique integer identifier for each input file. The numbering starts from `1`. |
# MAGIC | `input_file_path` | string | The full path to the input file. |
# MAGIC | `input_file_encoding` | string | The detected encoding of the input file (e.g., `UTF-8`). |
# MAGIC | `tokenizer_type` | string | The type of tokenizer used for token counting (e.g., `claude` or `openai`). |
# MAGIC | `tokenizer_model` | string | The specific model or encoding used for tokenization (e.g., `claude` for Claude models or `o200k_base` for OpenAI models). |
# MAGIC | `input_file_token_count` | int | The total number of tokens in the input file. |
# MAGIC | `input_file_token_count_without_sql_comments` | int | The number of tokens in the input file excluding SQL comments. |
# MAGIC | `input_file_content` | string | The entire content of the input file. |
# MAGIC | `input_file_content_without_sql_comments` | string | The content of the input file excluding SQL comments. |
# MAGIC | `is_conversion_target` | boolean | Indicates whether the file is a conversion target (True or False). This is determined in `01_analyze_input_files` based on a comparison between the token count of the input file (excluding SQL comments) and the `token_count_threshold`. It is automatically updated from `True` to `False` once the conversion process is successfully completed. |
# MAGIC | `model_serving_endpoint_for_conversion` | string | The model serving endpoint for the conversion process. |
# MAGIC | `model_serving_endpoint_for_fix` | string | The model serving endpoint for syntax error fixing. |
# MAGIC | `request_params_for_conversion` | string | Request parameters for the conversion process in JSON format. |
# MAGIC | `request_params_for_fix` | string | Request parameters for syntax error fixing in JSON format. |
# MAGIC | `result_content` | string | The converted content of the file after processing. (Initially `null`) |
# MAGIC | `result_prompt_tokens` | int | The number of prompt tokens used for the conversion. (Initially `null`) |
# MAGIC | `result_completion_tokens` | int | The number of completion tokens generated by the model. (Initially `null`) |
# MAGIC | `result_total_tokens` | int | The total number of tokens (prompt + completion) used for the conversion. (Initially `null`) |
# MAGIC | `result_processing_time_seconds` | float | The time in seconds it took to process the conversion request. (Initially `null`) |
# MAGIC | `result_timestamp` | timestamp | The timestamp (UTC) when the `result_content` was generated or updated. (Initially `null`) |
# MAGIC | `result_error` | string | Any errors encountered during the conversion process. (Initially `null`) |
# MAGIC | `result_python_parse_error` | string | Any errors encountered during the Python function syntax check using `ast.parse`. |
# MAGIC | `result_extracted_sqls` | array<string> | The list of SQL statements extracted from the Python function. (Initially `null`) |
# MAGIC | `result_sql_parse_errors` | array<string> | Any errors encountered during the SQL syntax check using `EXPLAIN sql`. (Initially `null`) |
# MAGIC
# MAGIC ## üîÑ How to Re-convert Specific Files
# MAGIC If the conversion result is not satisfactory, you can re-convert specific files by following these steps:
# MAGIC
# MAGIC 1.  Use the <a href="$./11_adjust_conversion_targets" target="_blank">11_adjust_conversion_targets</a> notebook to set the `is_conversion_target` field to `True` for the files you want to re-convert.
# MAGIC 2.  Re-run the <a href="$./02_convert_sql_to_databricks" target="_blank">02_convert_sql_to_databricks</a> and subsequent processes. Only the files marked as `is_conversion_target` with `True` will be re-converted.
# MAGIC     - To introduce more randomness in the LLM's conversion process and obtain different results on each run, it is recommended to set the `temperature` in `request_params` to above 0.5 if the model being used supports it.
# MAGIC
# MAGIC ## üíª Verified Environments
# MAGIC This notebook has been verified to work in the following environments:
# MAGIC
# MAGIC 1. Databricks serverless compute for notebooks and jobs
# MAGIC 2. Databricks classic compute (All-purpose compute and Jobs compute)
# MAGIC     - Recommended settings: Single-node cluster, Photon is not required
# MAGIC     - Verified Databricks Runtime (DBR) version
# MAGIC         - 15.3 LTS
# MAGIC         - 14.3 LTS

# COMMAND ----------

# MAGIC %md
# MAGIC ## 0. Set Up Configuration Parameters
# MAGIC Major configuration parameters are set up in this section. If you need to change other parameters, change then in the respective notebooks.

# COMMAND ----------

# DBTITLE 1,Install Packages
# MAGIC %pip install -r requirements.txt
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,Import Libraries
import json

import pandas as pd
from pyscripts.conversion_prompt_helper import ConversionPromptHelper
from pyscripts.databricks_credentials import DatabricksCredentials
from pyscripts.notebook_i18n import get_supported_languages

# COMMAND ----------

# DBTITLE 1,Configurations
# Params for 01_analyze_input_files
dbutils.widgets.text("input_dir", "", "1-1. Input Directory")
dbutils.widgets.text("endpoint_name", "databricks-claude-3-7-sonnet", "1-2. Serving Endpoint Name")
dbutils.widgets.text("result_catalog", "", "1-3. Result Catalog")
dbutils.widgets.text("result_schema", "", "1-4. Result Schema")
dbutils.widgets.text("token_count_threshold", "20000", "1-5. Token Count Threshold")
dbutils.widgets.text("existing_result_table", "", "Existing Result Table (Optional)")

# Params for 02_convert_sql_to_databricks
dbutils.widgets.dropdown("sql_dialect", "tsql", ConversionPromptHelper.get_supported_sql_dialects(), "2-1. SQL Dialect")
dbutils.widgets.dropdown("comment_lang", "English", get_supported_languages(), "2-2. Comment Language")
dbutils.widgets.text("concurrency", "4", "2-3. Concurrency Requests")
dbutils.widgets.dropdown("log_level", "INFO", ["DEBUG", "INFO"], "2-4. Log Level")
dbutils.widgets.text("request_params", "", "Chat Request Params (Optional)")
dbutils.widgets.text("conversion_prompt_yaml", "", "YAML path for Conversion Prompt (Optional)")

# Params for 03_syntax_check_and_fix
dbutils.widgets.text("max_fix_attempts", "1", "3-1. Maximum Fix Attempts")

# Params for 05_export_to_databricks_notebooks
dbutils.widgets.text("output_dir", "", "5-1. Output Directory")

# Params for experimental SQL notebook conversion
dbutils.widgets.text("sql_output_dir", "", "6-1. SQL Notebook Output Directory (Experimental)")

# COMMAND ----------

# DBTITLE 1,Load Configurations
input_dir = dbutils.widgets.get("input_dir")
endpoint_name = dbutils.widgets.get("endpoint_name")
result_catalog = dbutils.widgets.get("result_catalog")
result_schema = dbutils.widgets.get("result_schema")
token_count_threshold = int(dbutils.widgets.get("token_count_threshold"))
existing_result_table = dbutils.widgets.get("existing_result_table")
comment_lang = dbutils.widgets.get("comment_lang")
concurrency = int(dbutils.widgets.get("concurrency"))
request_params = dbutils.widgets.get("request_params")
log_level = dbutils.widgets.get("log_level")
max_fix_attempts = int(dbutils.widgets.get("max_fix_attempts"))
output_dir = dbutils.widgets.get("output_dir")
sql_output_dir = dbutils.widgets.get("sql_output_dir")

# Determin whith conversion YAML to use
_conversion_prompt_yaml = dbutils.widgets.get("conversion_prompt_yaml")
sql_dialect = dbutils.widgets.get("sql_dialect")

if _conversion_prompt_yaml:
    conversion_prompt_yaml = _conversion_prompt_yaml
else:
    conversion_prompt_yaml = ConversionPromptHelper.get_default_yaml_for_sql_dialect(sql_dialect)

input_dir, endpoint_name, result_catalog, result_schema, token_count_threshold, existing_result_table, conversion_prompt_yaml, comment_lang, request_params, log_level, max_fix_attempts, output_dir, sql_output_dir

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Analyze Input Files
# MAGIC Analyzes the input SQL files, calculates token counts, and saves the results to a Delta table.

# COMMAND ----------

# DBTITLE 1,Analyze Input Files
result_table = dbutils.notebook.run("01_analyze_input_files", 0, {
    "input_dir": input_dir,
    "endpoint_name": endpoint_name,
    "result_catalog": result_catalog,
    "result_schema": result_schema,
    "token_count_threshold": token_count_threshold,
    "existing_result_table": existing_result_table,
})
print(f"Conversion result table: {result_table}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Files Selected for Conversion
# MAGIC Files within token threshold: Will be converted to Databricks notebooks.

# COMMAND ----------

# DBTITLE 1,Files Selected for Conversion
spark.sql(f"""
    SELECT 
        input_file_number,
        input_file_path,
        input_file_token_count_without_sql_comments
    FROM {result_table}
    WHERE is_conversion_target = true
    ORDER BY input_file_number
""").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Files Exceeding Token Threshold
# MAGIC Files exceeding threshold: Need manual review. (Consider splitting into smaller files)

# COMMAND ----------

# DBTITLE 1,Files Exceeding Token Threshold
spark.sql(f"""
    SELECT 
        input_file_number,
        input_file_path,
        input_file_token_count_without_sql_comments
    FROM {result_table}
    WHERE is_conversion_target = false
    ORDER BY input_file_number
""").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Convert SQL to Databricks
# MAGIC Converts the SQL code to a Python function that runs in a Databricks notebook using an LLM and updates the result table.

# COMMAND ----------

# DBTITLE 1,Convert SQL to Databricks Notebooks
dbutils.notebook.run("02_convert_sql_to_databricks", 0, {
    "endpoint_name": endpoint_name,
    "result_table": result_table,
    "conversion_prompt_yaml": conversion_prompt_yaml,
    "comment_lang": comment_lang,
    "concurrency": concurrency,
    "request_params": request_params,
    "log_level": log_level,
})

# COMMAND ----------

# MAGIC %md
# MAGIC ### Successfully Converted Files
# MAGIC The following table shows files that have been successfully converted to Databricks notebooks.

# COMMAND ----------

# DBTITLE 1,Successfully Converted Files
spark.sql(f"""
    SELECT 
        input_file_number,
        input_file_path,
        result_content,
        input_file_token_count_without_sql_comments,
        result_prompt_tokens,
        result_completion_tokens,
        result_total_tokens,
        result_timestamp
    FROM {result_table}
    WHERE is_conversion_target = false
        AND result_content IS NOT NULL
    ORDER BY input_file_number
""").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Files with Conversion Errors
# MAGIC The following table shows files that have conversion errors.

# COMMAND ----------

# DBTITLE 1,Files with Conversion Errors
spark.sql(f"""
    SELECT 
        input_file_number,
        input_file_path,
        result_error,
        result_timestamp
    FROM {result_table}
    WHERE is_conversion_target = true
        AND result_error IS NOT NULL
    ORDER BY input_file_number
""").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Syntax Check and Fix
# MAGIC Performs static syntax checks on Python functions and the Spark SQL contained within them, and attempts to fix any errors found.

# COMMAND ----------

# DBTITLE 1,Function for Syntax Error File Count


def get_error_file_count(result_table: str) -> int:
    """Get the count of files with syntax errors."""
    error_count = spark.sql(f"""
        SELECT COUNT(*) as error_count
        FROM {result_table}
        WHERE result_python_parse_error IS NOT NULL
        OR (result_sql_parse_errors IS NOT NULL AND size(result_sql_parse_errors) > 0)
    """).collect()[0]['error_count']
    return error_count

# COMMAND ----------


# DBTITLE 1,Check and Fix Syntax Errors
for attempt in range(max_fix_attempts):
    # Run static syntax check
    print(f"Attempt {attempt + 1} of {max_fix_attempts}")
    dbutils.notebook.run("03_01_static_syntax_check", 0, {
        "result_table": result_table,
    })

    # Check if there are any errors
    error_count = get_error_file_count(result_table)
    if error_count == 0:
        print("No syntax errors found. Exiting fix loop.")
        break

    # Run fix syntax error
    print(f"Found {error_count} files with syntax errors. Attempting to fix...")
    dbutils.notebook.run("03_02_fix_syntax_error", 0, {
        "endpoint_name": endpoint_name,
        "result_table": result_table,
        "concurrency": concurrency,
        "request_params": request_params,
        "log_level": log_level,
    })

# COMMAND ----------

# MAGIC %md
# MAGIC ### Final Syntax Check
# MAGIC Performs a final static syntax check after all fix attempts.

# COMMAND ----------

# DBTITLE 1,Run Final Syntax Check
dbutils.notebook.run("03_01_static_syntax_check", 0, {
    "result_table": result_table,
})
error_count = get_error_file_count(result_table)
print(f"Found {error_count} files with syntax errors.")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Syntax Check Results
# MAGIC The following table shows the syntax check results for all files, including both successful and failed checks.

# COMMAND ----------

# DBTITLE 1,Syntax Check Status
spark.sql(f"""
    SELECT 
        input_file_number,
        input_file_path,
        result_content,
        CASE 
            WHEN result_python_parse_error IS NULL 
                AND (result_sql_parse_errors IS NULL OR size(result_sql_parse_errors) = 0)
            THEN 'No errors'
            ELSE 'Has errors'
        END as check_status,
        result_python_parse_error,
        result_sql_parse_errors
    FROM {result_table}
    ORDER BY input_file_number
""").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Split Cells
# MAGIC Splits the converted Python code into multiple cells based on logical structure and control flow.

# COMMAND ----------

# DBTITLE 1,Split Cells
dbutils.notebook.run("04_split_cells", 0, {
    "result_table": result_table,
    "log_level": log_level,
})

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Export to Databricks Notebooks
# MAGIC Exports the converted code to Databricks notebooks.

# COMMAND ----------

# DBTITLE 1,Export to Databricks Notebooks
export_results_json = dbutils.notebook.run("05_export_to_databricks_notebooks", 0, {
    "result_table": result_table,
    "output_dir": output_dir,
    "comment_lang": comment_lang
})

# COMMAND ----------

# MAGIC %md
# MAGIC %md
# MAGIC ### Results Summary
# MAGIC The following table shows the conversion and export results for all input SQL files.

# COMMAND ----------

# DBTITLE 1,Conversion and Export Status
# Display output directory URL
full_url = f"{DatabricksCredentials().host}#workspace{output_dir}"
displayHTML(f'<p><strong>Output directory URL: </strong><a href="{full_url}" target="_blank">{full_url}</a></p>')

# Create a temporary view of export results
export_results = json.loads(export_results_json)
export_results_df = pd.DataFrame(export_results)
spark.createDataFrame(export_results_df).createOrReplaceTempView("temp_export_results")

# Display complete status for all files
spark.sql(f"""
    SELECT 
        r.input_file_number,
        r.input_file_path,
        CASE 
            WHEN r.result_content IS NULL THEN 'Not converted'
            WHEN r.result_python_parse_error IS NOT NULL OR 
                 (r.result_sql_parse_errors IS NOT NULL AND size(r.result_sql_parse_errors) > 0)
            THEN 'Converted with errors'
            ELSE 'Converted successfully'
        END as conversion_status,
        CASE 
            WHEN t.output_file_path IS NOT NULL THEN 'Exported successfully'
            ELSE 'Not exported'
        END as export_status,
        t.output_file_path,
        t.parse_error_count,
        r.result_python_parse_error as python_errors,
        r.result_sql_parse_errors as sql_errors
    FROM {result_table} r
    LEFT JOIN temp_export_results t
    ON r.input_file_path = t.input_file_path
    ORDER BY r.input_file_number
""").display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Convert to SQL Notebooks (Experimental)
# MAGIC Optionally converts Python notebooks to SQL notebooks if SQL output directory is specified.

# COMMAND ----------

# DBTITLE 1,Convert to SQL Notebooks (if requested)
if sql_output_dir:
    print("Converting Python notebooks to SQL notebooks...")
    dbutils.notebook.run("06_convert_to_sql_notebooks", 0, {
        "python_input_dir": output_dir,
        "sql_output_dir": sql_output_dir,
        "endpoint_name": endpoint_name,
        "concurrency": concurrency,
        "request_params": request_params,
        "comment_lang": comment_lang,
        "log_level": log_level,
    })
    print("SQL notebook conversion completed.")
else:
    print("SQL notebook conversion skipped (sql_output_dir not specified).")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Next Steps
# MAGIC The SQL to Databricks conversion process is now complete. The results of this process are available in the converted notebooks located in the specified output directory. It's crucial to review these results thoroughly to ensure the converted code meets your requirements and functions as expected in the Databricks environment.
# MAGIC
# MAGIC When reviewing the results, pay special attention to the following cases and take appropriate action:
# MAGIC
# MAGIC 1. Files with `Not converted` status:
# MAGIC    - These input files couldn't be processed, often due to exceeding the token count threshold.
# MAGIC    - Action: Consider splitting these input files into smaller, more manageable parts. You may also try increasing the `token_count_threshold` parameter if your LLM model can handle larger inputs, then re-run the conversion process.
# MAGIC
# MAGIC 2. Files with `Converted with errors` status:
# MAGIC    - These files were converted but contain syntax errors.
# MAGIC    - Action: Review the syntax error messages at the bottom of the output notebooks. Manually fix these errors in the converted notebooks.
# MAGIC
# MAGIC 3. Files with `Not exported` status:
# MAGIC    - This status is rare but may occur if the converted content is too large.
# MAGIC    - If you see a "Content size exceeds 10MB limit" message in the export process, it indicates that the input file might be too large.
# MAGIC    - Action: Review and potentially reduce the size of the input SQL file, then try the conversion process again.
