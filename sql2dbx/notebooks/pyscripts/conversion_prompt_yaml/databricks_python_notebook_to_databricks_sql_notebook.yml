system_message: |
  Convert Databricks Python notebook code to Databricks SQL notebook code.

  ## General Conversion Rules:
  - Convert `spark.sql(f"...")` calls to direct SQL statements
  - Handle widget parameters based on usage:
    - For simple, static queries: Use parameter markers `:parameter_name`
    - For dynamic SQL or when values need manipulation: Use `DECLARE OR REPLACE VARIABLE` with DEFAULT values matching the widget defaults
  - When building dynamic SQL that needs to be executed:
    - Construct the SQL string using variables and `||` concatenation operator
    - Execute using `EXECUTE IMMEDIATE`
  - For dynamic table references, use `IDENTIFIER()` function with the full table path
  - Use `CONCAT_WS('.',...)` for building database.schema.table paths cleanly
  - Remove all Python code including imports, function definitions, and loops
  - Organize notebook cells with `-- COMMAND ----------` to separate logical units

  ## Specific Conversion Patterns:
  - Timezone handling (JST = UTC+9): Convert to `DATEADD(HOUR, 9, CURRENT_TIMESTAMP())`
  - Date formatting: Convert `strftime()` to `DATE_FORMAT()` with SQL format strings
  - Print statements: Convert to `SELECT ... AS status` to show progress messages
  - Conditional logic: Convert Python if/else to SQL CASE WHEN expressions
  - Multi-line dynamic SQL strings: Use triple quotes `"""` for readability

few_shots:
- role: user
  content: |
    # Databricks notebook source
    from datetime import datetime, timedelta, timezone
    
    # Databricks widgets for parameters
    dbutils.widgets.text("database", "example_db", "Database")
    dbutils.widgets.text("schema", "example_schema", "Schema")
    dbutils.widgets.text("source_table", "sales_data", "Source Table")
    dbutils.widgets.text("temp_table", "sales_temp", "Temp Table")
    
    def tm_print(msg):
        print(f"{datetime.now()} {msg}")
    
    tm_print("START")
    
    # Get parameters from widgets
    database = dbutils.widgets.get("database")
    schema = dbutils.widgets.get("schema")
    source_table = dbutils.widgets.get("source_table")
    temp_table = dbutils.widgets.get("temp_table")
    
    # Generate date key in JST timezone
    JST = timezone(timedelta(hours=9))
    now_jst = datetime.now(JST)
    date_key_today = now_jst.strftime("%Y%m%d")
    
    # Execute merge operation
    merge_sql = f"""
        MERGE INTO {database}.{schema}.{temp_table} as t1
        USING (
            select DATE_KEY, HOUR_KEY, PRODUCT_KEY, SALE_AMT
            from {database}.{schema}.{source_table}
            where DATE_KEY = {date_key_today}
        ) as t2
        ON t1.DATE_KEY = t2.DATE_KEY AND t1.PRODUCT_KEY = t2.PRODUCT_KEY
        WHEN NOT MATCHED THEN
            INSERT (DATE_KEY, HOUR_KEY, PRODUCT_KEY, SALE_AMT)
            VALUES (t2.DATE_KEY, t2.HOUR_KEY, t2.PRODUCT_KEY, t2.SALE_AMT)
    """
    spark.sql(merge_sql)
    tm_print("MERGE COMPLETE")
- role: assistant
  content: |
    -- Databricks notebook source
    -- Parameters: :database, :schema, :source_table, :temp_table
    
    SELECT CONCAT(CURRENT_TIMESTAMP(), ' START') AS status;
    
    -- COMMAND ----------
    
    -- Get current date key (JST timezone = UTC+9)
    DECLARE OR REPLACE VARIABLE date_key_today STRING;
    SET VARIABLE date_key_today = DATE_FORMAT(DATEADD(HOUR, 9, CURRENT_TIMESTAMP()), 'yyyyMMdd');
    
    -- Execute MERGE operation
    MERGE INTO IDENTIFIER(:database || '.' || :schema || '.' || :temp_table) as t1
    USING (
        select DATE_KEY, HOUR_KEY, PRODUCT_KEY, SALE_AMT
        from IDENTIFIER(:database || '.' || :schema || '.' || :source_table)
        where DATE_KEY = date_key_today
    ) as t2
    ON t1.DATE_KEY = t2.DATE_KEY AND t1.PRODUCT_KEY = t2.PRODUCT_KEY
    WHEN NOT MATCHED THEN
        INSERT (DATE_KEY, HOUR_KEY, PRODUCT_KEY, SALE_AMT)
        VALUES (t2.DATE_KEY, t2.HOUR_KEY, t2.PRODUCT_KEY, t2.SALE_AMT);
    
    SELECT CONCAT(CURRENT_TIMESTAMP(), ' MERGE COMPLETE') AS status;
- role: user
  content: |
    # Databricks notebook source
    import pandas as pd
    
    # Get parameter
    table_name = dbutils.widgets.get("table_name")
    
    # Execute simple query
    result = spark.sql(f"SELECT * FROM {table_name} WHERE date >= '2024-01-01'")
    display(result)
- role: assistant
  content: |
    -- Databricks notebook source
    -- Parameters: :table_name
    
    SELECT * FROM IDENTIFIER(:table_name) 
    WHERE date >= '2024-01-01';