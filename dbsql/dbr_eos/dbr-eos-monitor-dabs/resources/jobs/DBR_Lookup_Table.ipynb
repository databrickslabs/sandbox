{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a3319b-64e4-4039-9506-c800d5585232",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import beautifulsoup to scrape Databricks DBR doc page"
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fdeabb-0072-4a17-908c-c3f0010911ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d35ce2e-c0a6-414e-a724-38f59b5b152b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"wn_catalog\", \"dbdemos\")\n",
    "dbutils.widgets.text(\"wn_schema\", \"lookup\")\n",
    "dbutils.widgets.text(\"wn_table\", \"dbr_version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02227386-432b-4815-ae24-099afa5a1ca8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "We will use widgets to create the dbr lookup table name"
    }
   },
   "outputs": [],
   "source": [
    "# Get widget values\n",
    "wn_catalog = dbutils.widgets.get(\"wn_catalog\")\n",
    "wn_schema = dbutils.widgets.get(\"wn_schema\")\n",
    "wn_table = dbutils.widgets.get(\"wn_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826a3c4c-3a12-4d3d-8c88-8a35604e9716",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql.functions import to_date, coalesce, regexp_extract, when, col, try_to_timestamp, lit\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7384208-251f-4fcc-9b08-cbb5d6499551",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create catalog if not exists"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the catalog already exists. More elaborate way of checking to prevent error if max catalogs have already been created\n",
    "existing_catalogs = spark.sql(f\"SHOW CATALOGS LIKE '{wn_catalog}'\")\n",
    "if existing_catalogs.count() == 0:\n",
    "    create_catalog_sql = f\"CREATE CATALOG IF NOT EXISTS {wn_catalog}\"\n",
    "    spark.sql(create_catalog_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1de179-501f-4fcf-98ab-33d4d8baba18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create schema if not exists"
    }
   },
   "outputs": [],
   "source": [
    "create_schema_sql = f\"CREATE SCHEMA IF NOT EXISTS {wn_catalog}.{wn_schema}\"\n",
    "spark.sql(create_schema_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1224678-1f2f-45b9-9404-649bc9380b6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DBR Lookup Table Name"
    }
   },
   "outputs": [],
   "source": [
    "full_table_name = f\"{wn_catalog}.{wn_schema}.{wn_table}\"\n",
    "print(full_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ae6583-2b65-4f46-a41d-56e83dcffcdd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get DBR versions and end of life details from Databricks docs"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Get the page content\n",
    "url = 'https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/'\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the specific section by heading ID\n",
    "target_heading = soup.find('h2', {'id': 'all-supported-databricks-runtime-releases'})\n",
    "\n",
    "# Initialize variables for table extraction\n",
    "extracted_table = None\n",
    "\n",
    "if target_heading:\n",
    "    # Find the next table sibling after the target heading\n",
    "    target_table = target_heading.find_next('table')\n",
    "    \n",
    "    if target_table:\n",
    "        # Clean and normalize headers for Delta Lake compatibility\n",
    "        headers = [\n",
    "            th.text.strip()\n",
    "            .lower()  # Convert to lowercase\n",
    "            .replace(' ', '_')  # Replace spaces\n",
    "            .replace('-', '_')  # Replace hyphens\n",
    "            for th in target_table.find_all('th')\n",
    "        ]\n",
    "        \n",
    "        # Extract table rows\n",
    "        rows = []\n",
    "        for row in target_table.find_all('tr'):\n",
    "            row_data = [td.text.strip() for td in row.find_all('td')]\n",
    "            if row_data:  # Skip header row\n",
    "                rows.append(row_data)\n",
    "        \n",
    "        # Create result structure\n",
    "        extracted_table = {\n",
    "            'headers': headers,\n",
    "            'data': rows\n",
    "        }\n",
    "\n",
    "display(extracted_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b31ed0-40f8-4455-b022-f23389958581",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create DataFrame with dates formatted"
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.createDataFrame(extracted_table['data'], extracted_table['headers'])\n",
    "    .withColumnRenamed('version', 'dbr_version')\n",
    "    .withColumnRenamed('variants', 'dbr_variants')\n",
    "    .withColumn('clean_dbr_version', regexp_extract(col('dbr_version'), r'\\d+\\.\\d', 0))\n",
    "    .withColumn(\n",
    "        'release_date',\n",
    "        coalesce(\n",
    "            try_to_timestamp(col('release_date'), lit('MMM d, yyyy')).cast('date'),\n",
    "            try_to_timestamp(col('release_date'), lit('MMMM d, yyyy')).cast('date')\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\n",
    "        'end_of_support_date',\n",
    "        coalesce(\n",
    "            try_to_timestamp(col('end_of_support_date'), lit('MMM d, yyyy')).cast('date'),\n",
    "            try_to_timestamp(col('end_of_support_date'), lit('MMMM d, yyyy')).cast('date')\n",
    "        )\n",
    "    )\n",
    "    .withColumn('dbr_is_lts', when(col('dbr_version').contains('LTS'), True).otherwise(False))\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe33912-0d76-4998-99b1-48530e737949",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "If this is the first run then write as Delta Table, else merge into the existing Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(full_table_name):\n",
    "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(full_table_name)\n",
    "else:\n",
    "    delta_table = DeltaTable.forName(spark, full_table_name)\n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         df.alias(\"source\"),\n",
    "         \"target.clean_dbr_version = source.clean_dbr_version\"\n",
    "     )\n",
    "     .whenMatchedUpdateAll()  # Update existing records\n",
    "     .whenNotMatchedInsertAll()  # Insert new records\n",
    "     .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77fad61-09e8-4847-a0aa-a6572bb9286d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Fake data and Run cell 10 again to demonstrate how we will Merge going forward. Remove for Production"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Create fake data for merging\n",
    "fake_data = [\n",
    "    # Row 1: Updates existing 16.2 record with new end-of-support date\n",
    "    [\"16.2\", \"Databricks Runtime 16.2Databricks Runtime 16.2 for Machine Learning\", \n",
    "     \"3.5.0\", \"Feb 5, 2025\", \"Oct 5, 2025\", \"16.2\", False],\n",
    "     \n",
    "    # Row 2: Inserts a completely new version (17.0)\n",
    "    [\"17.0\", \"Databricks Runtime 17.0Databricks Runtime 17.0 for Machine Learning\", \n",
    "     \"3.4.0\", \"Sep 15, 2024\", \"Mar 15, 2026\", \"17.0\", False]\n",
    "]\n",
    "\n",
    "# Define schema matching your original table\n",
    "columns = [\"dbr_version\", \"dbr_variants\", \"apache_spark_version\", \n",
    "          \"release_date\", \"end_of_support_date\", \"clean_dbr_version\", \"dbr_is_lts\"]\n",
    "\n",
    "# Create DataFrame with the fake data\n",
    "df = (spark.createDataFrame(fake_data, columns).withColumn('release_date', coalesce(to_date('release_date', 'MMM d, yyyy'), to_date('release_date', 'MMMM d, yyyy')))\n",
    "    .withColumn('end_of_support_date', coalesce(to_date('end_of_support_date', 'MMM d, yyyy'), to_date('end_of_support_date', 'MMMM d, yyyy')))\n",
    ") \n",
    "display(df)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1144435745009817,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DBR_Lookup_Table",
   "widgets": {
    "wn_catalog": {
     "currentValue": "dbdemos",
     "nuid": "6f293237-10d9-4540-8c05-aaeeaee0524f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbdemos",
      "label": "",
      "name": "wn_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbdemos",
      "label": "",
      "name": "wn_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "wn_schema": {
     "currentValue": "lookup",
     "nuid": "1c3648bb-4f00-4e4a-90bb-911baa16641d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "lookup",
      "label": "",
      "name": "wn_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "lookup",
      "label": "",
      "name": "wn_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "wn_table": {
     "currentValue": "dbr_version",
     "nuid": "ce7a9861-847d-4afb-876b-cbe12d3bb116",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbr_version",
      "label": "",
      "name": "wn_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbr_version",
      "label": "",
      "name": "wn_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
